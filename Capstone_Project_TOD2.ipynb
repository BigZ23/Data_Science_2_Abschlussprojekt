{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx1iySDZ+N3QFMqBubaqmX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippWiessner/DataScience2/blob/main/Capstone_Project_TOD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Capstone Project Geospace Maschine Learnig Model**"
      ],
      "metadata": {
        "id": "DKo2gZRiyEk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Content:\n",
        "Building a geospace Model to recognize different Usages of Land**"
      ],
      "metadata": {
        "id": "fbJaPrqAyIK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(description of our project / Reasons for choosing this project topic (City Planning through land usage recocnition)"
      ],
      "metadata": {
        "id": "71xiwe2Vy0p9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data generation**"
      ],
      "metadata": {
        "id": "eSJmyoqDISj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial phase in constructing a spatial-based model involves the collection of essential image data. To achieve this, we opted to use OpenStreetMaps, where images are pre-defined based on building types, road types, and more. This eliminates the need for manual scripting of every entity within each image, allowing us to concentrate on tasking of masking images for the learning process.\n",
        "\n",
        "For obtaining these images locally, we utilized a Google download function, necessitating the use of the Google Cloud service and a dedicated API key."
      ],
      "metadata": {
        "id": "rxQGULvczzyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to donwload images from the googlestaticmap api\n",
        "def download_images(coordinates, zoom=16):\n",
        "    # define the request parameters\n",
        "    url = \"https://maps.googleapis.com/maps/api/staticmap?\"\n",
        "    api_key = \"\"\n",
        "    size = \"640x640\"\n",
        "    scale = \"1\"\n",
        "    maptype = \"satellite\"\n",
        "\n",
        "    # pose get request\n",
        "    response = requests.get(url + \"center=\" + str(coordinates[0]) + \",\" + str(coordinates[1]) + \"&zoom=\" + str(zoom) + \"&size=\" + size + \"&maptype=\" + maptype + \"&scale=\" + scale + \"&sensor=false\" + \"&key=\" + api_key, stream=True,)\n",
        "    #stream repsonse into file and save it\n",
        "    with open(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\", \"wb\",) as out_file:\n",
        "        shutil.copyfileobj(response.raw, out_file)\n",
        "    #delete the repsonse arifact\n",
        "    del response"
      ],
      "metadata": {
        "id": "uLPMr46myYEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior to utilizing the image downloader, we decided to predefine specific areas and coordinates. This precautionary step ensures that we exclude images from regions that are of no importance for the training model, such as images from oceans or seas."
      ],
      "metadata": {
        "id": "nwgj2rG61Ryn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach involved taking a package that contains a list of all cities available in OpenStreetMaps, each with a population of 15,000 or more. This strategy was implemented to avoid irrelevant images and place a greater emphasis on the initial concept of city analysis."
      ],
      "metadata": {
        "id": "P443WEWj2SvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset of all cities in the world with >15000 inhabitants\n",
        "package = Package(\"https://datahub.io/core/world-cities/datapackage.json\")\n",
        "\n",
        "cities_list = package.get_resource(\"world-cities_csv\").read()"
      ],
      "metadata": {
        "id": "Xzhw_gdy2jlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch random city contained in the dataset\n",
        "def get_random_city():\n",
        "\n",
        "    city = random.choice(cities_list)\n",
        "\n",
        "    return city"
      ],
      "metadata": {
        "id": "4x_wrq252l87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coordinates are required in establishing the length and width of each image, ensuring equality in size across all images. The following function is designed to define these coordinates based on the nodes of the OpenStreetMap image."
      ],
      "metadata": {
        "id": "4ZfLIlJN22Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coordinates_from_id(osmid):\n",
        "\n",
        "    # connect to overpass api endpoind\n",
        "    api = overpy.Overpass()\n",
        "\n",
        "    # query the api and save the response\n",
        "    result = api.query(f\"node({osmid});out;\")\n",
        "\n",
        "    for node in result.nodes:\n",
        "        # extract coordinates from response\n",
        "        coordinates = (float(node.lat), float(node.lon))\n",
        "\n",
        "    # if coordinates are available return them\n",
        "    try:\n",
        "        return coordinates\n",
        "\n",
        "    #else print error\n",
        "    except:\n",
        "        print(f\"No coordinates for {osmid}\")"
      ],
      "metadata": {
        "id": "yOaQjSjD3nid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To further suit our images for the training process, we check their compatibility by scaling them to a standardized size commonly used for map image data."
      ],
      "metadata": {
        "id": "eOKdJ2uX4qKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def latLngToPoint(mapWidth, mapHeight, lat, lng):\n",
        "    x = (lng + 180) * (mapWidth / 360)\n",
        "\n",
        "    y = ((1 - math.log(math.tan(lat * math.pi / 180) + 1 / math.cos(lat * math.pi / 180)) / math.pi) / 2) * mapHeight\n",
        "\n",
        "    return (x, y)\n",
        "\n",
        "\n",
        "\n",
        "def pointToLatLng(mapWidth, mapHeight, x, y):\n",
        "    lng = x / mapWidth * 360 - 180\n",
        "    n = math.pi - 2 * math.pi * y / mapHeight\n",
        "    lat = 180 / math.pi * math.atan(0.5 * (math.exp(n) - math.exp(-n)))\n",
        "\n",
        "\n",
        "    return (lat, lng)\n",
        "\n",
        "\n",
        "\n",
        "def getImageBounds(lat, lng, zoom):\n",
        "    picHeight = 640\n",
        "    picWidth = 640\n",
        "\n",
        "\n",
        "    mapHeight = 256\n",
        "    mapWidth = 256\n",
        "\n",
        "\n",
        "    xScale = math.pow(2, zoom) / (picWidth / mapWidth)\n",
        "    yScale = math.pow(2, zoom) / (picHeight / mapWidth)\n",
        "\n",
        "\n",
        "    centreX, centreY = latLngToPoint(mapWidth, mapHeight, lat, lng)\n",
        "\n",
        "\n",
        "    southWestX = centreX - (mapWidth / 2) / xScale\n",
        "    southWestY = centreY + (mapHeight / 2) / yScale\n",
        "    SWlat, SWlng = pointToLatLng(mapWidth, mapHeight, southWestX, southWestY)\n",
        "\n",
        "\n",
        "    northEastX = centreX + (mapWidth / 2) / xScale\n",
        "    northEastY = centreY - (mapHeight / 2) / yScale\n",
        "    NElat, NElng = pointToLatLng(mapWidth, mapHeight, northEastX, northEastY)\n",
        "\n",
        "\n",
        "    return [SWlat, SWlng, NElat, NElng]"
      ],
      "metadata": {
        "id": "u4GgC11B4_cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all nessessary functions established, the next step involves defining a list that includes all relevant land-use types. Since each node of the image data has an assigned type, we can employ a pre-defined set for land usages to ease this process."
      ],
      "metadata": {
        "id": "QgQ9kf5X6HOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list with all landuses\n",
        "dictionary_landuse = [\n",
        "    \"commercial\",\n",
        "    \"construction\",\n",
        "    \"education\",\n",
        "    \"fairground\",\n",
        "    \"industrial\",\n",
        "    \"residential\",\n",
        "    \"retail\",\n",
        "    \"institutional\",\n",
        "    \"aquaculture\",\n",
        "    \"allotments\",\n",
        "    \"farmland\",\n",
        "    \"farmyard\",\n",
        "    \"paddy\",\n",
        "    \"animal_keeping\",\n",
        "    \"flowerbed\",\n",
        "    \"forest\",\n",
        "    \"greenhouse_horticulture\",\n",
        "    \"meadow\",\n",
        "    \"orchard\",\n",
        "    \"plant_nursery\",\n",
        "    \"vineyard\",\n",
        "    \"basin\",\n",
        "    \"salt_pond\",\n",
        "    \"brownfield\",\n",
        "    \"cemetery\",\n",
        "    \"depot\",\n",
        "    \"garages\",\n",
        "    \"grass\",\n",
        "    \"greenfield\",\n",
        "    \"landfill\",\n",
        "    \"military\",\n",
        "    \"port\",\n",
        "    \"quarry\",\n",
        "    \"railway\",\n",
        "    \"recreation_ground\",\n",
        "    \"religious\",\n",
        "    \"village_green\",\n",
        "    \"winter_sports\",\n",
        "]"
      ],
      "metadata": {
        "id": "xKecKSZ26fRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our decision was to train the model on 5000 images."
      ],
      "metadata": {
        "id": "-CD5kuTO6gNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate a set of images for n cities\n",
        "def get_random_samples(n=1):\n",
        "    # loop to do multiple cities\n",
        "\n",
        "    for x in range(n):\n",
        "\n",
        "        # get a random city\n",
        "        city = get_random_city()\n",
        "\n",
        "        # get the coordinates for one of all the different landuses\n",
        "        coordinates = get_coordinates_for_each_landuse(city)\n",
        "\n",
        "        try:\n",
        "\n",
        "            # loop over each coordinate\n",
        "            for coordinate in coordinates:\n",
        "\n",
        "                try:\n",
        "                    # check if data is actually present in the given area\n",
        "                    south, east, north, west = getImageBounds(\n",
        "                        float(coordinate[0]), float(coordinate[1]), 16\n",
        "                    )\n",
        "                    landuse = ox.features_from_bbox(\n",
        "                        north, south, east, west, tags={\"landuse\": True}\n",
        "                    )\n",
        "                    # download the image of the given coordinate\n",
        "                    download_images(coordinate)\n",
        "\n",
        "                except:\n",
        "                    print(\"No landuse data was found\")\n",
        "\n",
        "        except:\n",
        "            print(\"Coordinates were returned empty\")"
      ],
      "metadata": {
        "id": "VvBBwuQw6qw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial count of how much data is already in the folder\n",
        "datanames = os.listdir(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\")\n",
        "\n",
        "# loop as long as less then 5000 examples\n",
        "while len(datanames) < 5000:\n",
        "\n",
        "    get_random_samples()\n",
        "\n",
        "    datanames = os.listdir(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\")"
      ],
      "metadata": {
        "id": "NEz8B4cm6u1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Training the model**"
      ],
      "metadata": {
        "id": "H01sVphrIWlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Combining satellite and OpenStreetMap data**"
      ],
      "metadata": {
        "id": "e7DkvI5vPpZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have obtained the necessary image data, we shifts to training the model for our specific objectives. As the model identifies various landuse types present in our images, our initial step involves assigning a color for each different landuse type to ease the evaluation. Due to certain steps in the model training process not accepting Hexcode colors, we additional assigned each land-use type a unique integer identifier."
      ],
      "metadata": {
        "id": "JiTqNu9HIrNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map each landuse to a distinct color\n",
        "landuse_mapped_hex = {\n",
        "    \"not_assigned\": \"#FFFFFF\",\n",
        "    \"commercial\": \"#2F4F4F\",\n",
        "    \"construction\": \"#556B2F\",\n",
        "    \"education\": \"#A0522D\",\n",
        "    \"fairground\": \"#006400\",\n",
        "    \"industrial\": \"#8B0000\",\n",
        "    \"residential\": \"#808000\",\n",
        "    \"retail\": \"#483D8B\",\n",
        "    \"institutional\": \"#778899\",\n",
        "    \"aquaculture\": \"#BC8F8F\",\n",
        "    \"allotments\": \"#008B8B\",\n",
        "    \"farmland\": \"#00008B\",\n",
        "    \"farmyard\": \"#32CD32\",\n",
        "    \"paddy\": \"#DAA520\",\n",
        "    \"animal_keeping\": \"#8FB88F\",\n",
        "    \"flowerbed\": \"#8B008B\",\n",
        "    \"forest\": \"#B03060\",\n",
        "    \"greenhouse_horticulture\": \"#FF0000\",\n",
        "    \"meadow\": \"#FF8C00\",\n",
        "    \"orchard\": \"#FFFF00\",\n",
        "    \"plant_nursery\": \"#0000CD\",\n",
        "    \"vineyard\": \"#40E0D0\",\n",
        "    \"basin\": \"#00FF00\",\n",
        "    \"salt_pond\": \"#DC143C\",\n",
        "    \"brownfield\": \"#00BFFF\",\n",
        "    \"cemetery\": \"#A020F0\",\n",
        "    \"depot\": \"#F08080\",\n",
        "    \"garages\": \"#ADFF2F\",\n",
        "    \"grass\": \"#DA70D6\",\n",
        "    \"greenfield\": \"#FF7F50\",\n",
        "    \"landfill\": \"#FF00FF\",\n",
        "    \"military\": \"#F0E68C\",\n",
        "    \"port\": \"#6495ED\",\n",
        "    \"quarry\": \"#DDA0DD\",\n",
        "    \"railway\": \"#B0E0E6\",\n",
        "    \"recreation_ground\": \"#90EE90\",\n",
        "    \"religious\": \"#FF1493\",\n",
        "    \"village_green\": \"#7B68EE\",\n",
        "    \"winter_sports\": \"#FFDAB9\",\n",
        "}"
      ],
      "metadata": {
        "id": "2el0fnSfK_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map each color to a integer value that later represents the individual ids\n",
        "color_mapped_to_int = {\n",
        "    \"255255255\": 0,\n",
        "    \"477979\": 1,\n",
        "    \"8510747\": 2,\n",
        "    \"1608245\": 3,\n",
        "    \"01000\": 4,\n",
        "    \"13900\": 5,\n",
        "    \"1281280\": 6,\n",
        "    \"7261139\": 7,\n",
        "    \"119136153\": 8,\n",
        "    \"188143143\": 9,\n",
        "    \"0139139\": 10,\n",
        "    \"00139\": 11,\n",
        "    \"5020550\": 12,\n",
        "    \"21816532\": 13,\n",
        "    \"143188143\": 14,\n",
        "    \"1390139\": 15,\n",
        "    \"1764896\": 16,\n",
        "    \"25500\": 17,\n",
        "    \"2551400\": 18,\n",
        "    \"2552550\": 19,\n",
        "    \"00205\": 20,\n",
        "    \"64224208\": 21,\n",
        "    \"02550\": 22,\n",
        "    \"2202060\": 23,\n",
        "    \"0191255\": 24,\n",
        "    \"16032240\": 25,\n",
        "    \"240128128\": 26,\n",
        "    \"17325547\": 27,\n",
        "    \"218112214\": 28,\n",
        "    \"25512780\": 29,\n",
        "    \"2550255\": 30,\n",
        "    \"240230140\": 31,\n",
        "    \"100149237\": 32,\n",
        "    \"221160221\": 33,\n",
        "    \"176224230\": 34,\n",
        "    \"144238144\": 35,\n",
        "    \"25520147\": 36,\n",
        "    \"123104238\": 37,\n",
        "    \"255218185\": 38,\n",
        "}"
      ],
      "metadata": {
        "id": "TFQWzlDYLFYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Up to this point, our images are essentially snapshots of map data. To integrate our land-usages into the maps, we developed a function that determines the size of the images and reads the land-use type of each node. It can then apply the corresponding color to each node, therefore generating the initial plot for one of our map images."
      ],
      "metadata": {
        "id": "rvcPxhWXL0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the openstreetmap data and save it as a .png\n",
        "def plot_data(coordinates, zoom):\n",
        "    # get the bounding box form the given coordinates and zoom level\n",
        "    south, east, north, west = getImageBounds(\n",
        "        float(coordinates[0]), float(coordinates[1]), zoom\n",
        "    )\n",
        "\n",
        "    # query openstreetmap for all landuse in the boundingbox\n",
        "    landuse = ox.features_from_bbox(north, south, east, west, tags={\"landuse\": True})\n",
        "\n",
        "    # create a plot\n",
        "    fig, ax = plt.subplots(figsize=(8, 8), dpi=104)\n",
        "\n",
        "    # for each landuse in the queried data, color the respective area in the earlier specified color\n",
        "    for type in landuse[\"landuse\"].unique():\n",
        "        # filter so wrongly labeled data doesn't cause any issues (instances of this can be seen on the openstreetmap key wiki)\n",
        "        if type in dictionary_landuse:\n",
        "            # filter for the currently selected landuse and plot it.\n",
        "            # turning of antialising and edgecolor is important as to not cause the creation of new colors on the edges of the areas that weren't assigned to a landuse\n",
        "            landuse.loc[landuse[\"landuse\"] == type].plot(\n",
        "                ax=ax,\n",
        "                color=landuse_mapped_hex[type],\n",
        "                antialiased=False,\n",
        "                edgecolor=\"none\",\n",
        "            )\n",
        "    plt.xlim(east, west)\n",
        "    plt.ylim(south, north)\n",
        "    # turn of the axis, as to not save it in the image file\n",
        "    plt.axis(\"off\")\n",
        "    # save the image file under the specified path\n",
        "    fig.savefig(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\", bbox_inches=\"tight\", pad_inches=0,)\n",
        "    # close the plot to reduce memory usage\n",
        "    plt.close()\n",
        "    # open the image again\n",
        "    rgb_converted = Image.open(os.getcwd()+ os.sep+ \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")\n",
        "    # convert it from rgba to rgb as we are not using the alpha values\n",
        "    rgb_converted = rgb_converted.convert(\"RGB\")\n",
        "    # save it again\n",
        "    rgb_converted.save(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")"
      ],
      "metadata": {
        "id": "2HkVdoGyNB48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The masks required for the eventual training of the model are based on these initial plots mentioned above. With similar aproaches involving defining bounds and color distribution, we create the mask to our initial map image plot."
      ],
      "metadata": {
        "id": "dI66qPMONXeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create numpy masks form the .png masks\n",
        "def generate_mask(coordinates):\n",
        "    # create 640x640 array filled with zeros\n",
        "    raster = np.zeros((640, 640))\n",
        "    # load the specified image file\n",
        "    image = Image.open(\n",
        "        os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")\n",
        "    # convert the image to a numpy array for way faster processing times (2min vs 1sec for mask creation)\n",
        "    image_array = np.array(image)\n",
        "    # go through each pixel and map the pixelvalue to the corresponding unique id\n",
        "    for line in range(640):\n",
        "        for column in range(640):\n",
        "            # get the value of the pixel in the image\n",
        "            pixelvalue = image_array[line, column]\n",
        "            # convert it to a string, because arrays can't be keys of dicitionaries, the easiest way to map the pixelvalues is by using their value as a string as keys\n",
        "            pixelvalue = map(str, pixelvalue)\n",
        "            colorvalue = \"\".join(pixelvalue)\n",
        "            # map the pixelvalue to the correspoding id\n",
        "            raster[line, column] = color_mapped_to_int[str(colorvalue)]\n",
        "    # save the numpy array using savez_compress to reduce the used storage space (3000 kb vs 10kb)\n",
        "    np.savez_compressed(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".npz\", raster,)\n",
        "    # delete the used picture\n",
        "    os.remove(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")"
      ],
      "metadata": {
        "id": "KDBYeFNSOqT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can then be looped through all the images available. In our case this porcess runs 5000 times."
      ],
      "metadata": {
        "id": "TRCxjKoHOuE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to loop over all satellite pictures and generate their masks\n",
        "def create_all_masks():\n",
        "    # get all filenames in Data/Images\n",
        "    datanames = os.listdir(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\")\n",
        "    # loop over all names\n",
        "    for dataname in datanames:\n",
        "        # extract coordinates out of filename\n",
        "        coordinates = dataname.rstrip(\".png\").split(\"_\")\n",
        "        # check if numpy data already exists\n",
        "        if not os.path.exists(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".npz\"):\n",
        "            coordinates = tuple(coordinates)\n",
        "            try:\n",
        "                # plot the data\n",
        "                plot_data(coordinates, 16)\n",
        "                # generate the numpy mask\n",
        "                generate_mask(coordinates)\n",
        "            except:\n",
        "                # if errors occur delte the problematic satellite image\n",
        "                print(\"An error occured while trying to generate the masks\")\n",
        "                os.remove( os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")"
      ],
      "metadata": {
        "id": "FYIt5DHUPSPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_all_masks()"
      ],
      "metadata": {
        "id": "DyQKYwdYPUsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2. Creating the Dataloader**"
      ],
      "metadata": {
        "id": "T_t5PAvMPz62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataloader process aligns to that from the lecture. Key points in our case are:\n",
        "\n",
        "\n",
        "1.  **Mask Restucturing**: It is essential to reorganize the masks stored as numpy files to make them compatible with the datablock's y_coordinate.\n",
        "2.   **Data Splitter**: We decided on using the RandomSplitter to split our images into train and test data.\n",
        "3.   **GPU Setup**: Due to using pytorch and cuda we can manually setup the process to be ran on the GPU instead of the CPU\n",
        "\n"
      ],
      "metadata": {
        "id": "Bn3kktVISkHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get the mask given an image\n",
        "def get_mask(x):\n",
        "    # load the numpyfile\n",
        "    numpyfile = np.load(mask_path / f\"{x.stem}.npz\")\n",
        "    # access the data stored in the compressed file format\n",
        "    data = numpyfile.f.arr_0\n",
        "    # close the file as to save memory\n",
        "    numpyfile.close()\n",
        "    # return the extracted mask\n",
        "    return data"
      ],
      "metadata": {
        "id": "nMbuPbCKTPiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the datablock\n",
        "satellite_block = DataBlock(\n",
        "    blocks=(ImageBlock, MaskBlock(codes=dictionary_landuse)),\n",
        "    get_items=get_image_files,\n",
        "    get_y=get_mask,\n",
        "    splitter=RandomSplitter(),\n",
        "    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n",
        "    batch_tfms=aug_transforms(),\n",
        ")"
      ],
      "metadata": {
        "id": "C9q3GWQ5UBrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if pytorch and cuda is setup correctly enable the dataloader to run on the gpu by passing it the cuda device (Installation of cuda https://developer.nvidia.com/cuda-downloads is necessary)\n",
        "if torch.cuda.is_available():\n",
        "    dataloader = satellite_block.dataloaders(\n",
        "        image_path, bs=32, device=torch.device(\"cuda\")\n",
        "    )\n",
        "\n",
        "\n",
        "else:\n",
        "    dataloader = satellite_block.dataloaders(image_path, bs=32)"
      ],
      "metadata": {
        "id": "a-uQHAxDUsul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Training the model"
      ],
      "metadata": {
        "id": "RPzZ8Kz9UOAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learner text"
      ],
      "metadata": {
        "id": "kVDSSV_tUxvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the unet learner, the metrics, also enable self attention, set mish as the activation function and pass ranger as the optimization function\n",
        "learner = unet_learner(\n",
        "    dataloader,\n",
        "    resnet34,\n",
        "    metrics=[dice_wo_bg, Dice],\n",
        "    self_attention=True,\n",
        "    act_cls=Mish,\n",
        "    opt_func=ranger,\n",
        ")"
      ],
      "metadata": {
        "id": "TmNaNeY9UwsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}