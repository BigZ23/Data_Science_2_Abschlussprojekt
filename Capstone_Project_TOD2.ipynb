{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippWiessner/DataScience2/blob/main/Capstone_Project_TOD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Capstone Project Geospace Maschine Learnig Model**"
      ],
      "metadata": {
        "id": "DKo2gZRiyEk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Content:\n",
        "Building a geospace Model to recognize different Usages of Land**"
      ],
      "metadata": {
        "id": "fbJaPrqAyIK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(description of our project / Reasons for choosing this project topic (City Planning through land usage recocnition)"
      ],
      "metadata": {
        "id": "71xiwe2Vy0p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Requirements\n",
        "\n",
        "#pip install datapackage\n",
        "from datapackage import Package\n",
        "import requests\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "\n",
        "#installation of requirements.txt\n",
        "\n",
        "# Replace this URL with the correct URL to your repository's raw requirements.txt\n",
        "requirements_url = \"https://raw.githubusercontent.com/PhilippWiessner/DataScience2/main/Model/requirements.txt\"\n",
        "\n",
        "# Fetch the content of requirements.txt\n",
        "response = requests.get(requirements_url)\n",
        "requirements_content = response.text\n",
        "\n",
        "# Write the content to a local requirements.txt file\n",
        "with open(\"requirements.txt\", \"w\") as file:\n",
        "    file.write(requirements_content)\n",
        "\n",
        "# Install requirements\n",
        "os.system(\"pip install -r requirements.txt\")\n"
      ],
      "metadata": {
        "id": "EKmb6yIejEnq",
        "outputId": "a66af16a-517f-433b-cbe0-8a364df47bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data generation**"
      ],
      "metadata": {
        "id": "eSJmyoqDISj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial phase in constructing a spatial-based model involves the collection of essential image data. To achieve this, we opted to use OpenStreetMaps, where images are pre-defined based on building types, road types, and more. This eliminates the need for manual scripting of every entity within each image, allowing us to concentrate on tasking of masking images for the learning process.\n",
        "\n",
        "For obtaining these images locally, we utilized a Google download function, necessitating the use of the Google Cloud service and a dedicated API key."
      ],
      "metadata": {
        "id": "rxQGULvczzyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"##\"  This function takes coordinates and an optional zoom level, sends a request to the Google Maps API, and saves the obtained image locally.\n",
        "The image file is stored with a filename based on its coordinates. The use of the Google Cloud service and API key ensures access to the required mapping data.\n",
        "This initial phase sets the foundation for subsequent steps in building the geospace machine learning model."
      ],
      "metadata": {
        "id": "IFvXTDSUqT4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to donwload images from the googlestaticmap api\n",
        "def download_images(coordinates, zoom=16):\n",
        "    # define the request parameters\n",
        "    url = \"https://maps.googleapis.com/maps/api/staticmap?\"\n",
        "    api_key = \"\"\n",
        "    size = \"640x640\"\n",
        "    scale = \"1\"\n",
        "    maptype = \"satellite\"\n",
        "\n",
        "    # pose get request\n",
        "    response = requests.get(url + \"center=\" + str(coordinates[0]) + \",\" + str(coordinates[1]) + \"&zoom=\" + str(zoom) + \"&size=\" + size + \"&maptype=\" + maptype + \"&scale=\" + scale + \"&sensor=false\" + \"&key=\" + api_key, stream=True,)\n",
        "    #stream repsonse into file and save it\n",
        "    with open(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\", \"wb\",) as out_file:\n",
        "        shutil.copyfileobj(response.raw, out_file)\n",
        "    #delete the repsonse arifact\n",
        "    del response"
      ],
      "metadata": {
        "id": "uLPMr46myYEu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior to utilizing the image downloader, we decided to predefine specific areas and coordinates. This precautionary step ensures that we exclude images from regions that are of no importance for the training model, such as images from oceans or seas."
      ],
      "metadata": {
        "id": "nwgj2rG61Ryn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach involved taking a package that contains a list of all cities available in OpenStreetMaps, each with a population of 15,000 or more. This strategy was implemented to avoid irrelevant images and place a greater emphasis on the initial concept of city analysis."
      ],
      "metadata": {
        "id": "P443WEWj2SvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bZuhsASymnLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset of all cities in the world with >15000 inhabitants\n",
        "# package = Package(\"https://datahub.io/core/world-cities/datapackage.json\")\n",
        "##Problem: DataPackageException: Unable to load JSON at \"https://datahub.io/core/world-cities/datapackage.json\"\n",
        "package = Package(\"https://raw.githubusercontent.com/datasets/world-cities/5eeadf346fe8ee2341597b4cb33531e85501c403/datapackage.json\")\n",
        "\n",
        "cities_list = package.get_resource(\"world-cities\").read()"
      ],
      "metadata": {
        "id": "Xzhw_gdy2jlM"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch random city contained in the dataset\n",
        "def get_random_city():\n",
        "\n",
        "    city = random.choice(cities_list)\n",
        "\n",
        "    return city"
      ],
      "metadata": {
        "id": "4x_wrq252l87"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coordinates are required in establishing the length and width of each image, ensuring equality in size across all images. The following function is designed to define these coordinates based on the nodes of the OpenStreetMap image."
      ],
      "metadata": {
        "id": "4ZfLIlJN22Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coordinates_from_id(osmid):\n",
        "\n",
        "    # connect to overpass api endpoind\n",
        "    api = overpy.Overpass()\n",
        "\n",
        "    # query the api and save the response\n",
        "    result = api.query(f\"node({osmid});out;\")\n",
        "\n",
        "    for node in result.nodes:\n",
        "        # extract coordinates from response\n",
        "        coordinates = (float(node.lat), float(node.lon))\n",
        "\n",
        "    # if coordinates are available return them\n",
        "    try:\n",
        "        return coordinates\n",
        "\n",
        "    #else print error\n",
        "    except:\n",
        "        print(f\"No coordinates for {osmid}\")"
      ],
      "metadata": {
        "id": "yOaQjSjD3nid"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To further suit our images for the training process, we check their compatibility by scaling them to a standardized size commonly used for map image data."
      ],
      "metadata": {
        "id": "eOKdJ2uX4qKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def latLngToPoint(mapWidth, mapHeight, lat, lng):\n",
        "    x = (lng + 180) * (mapWidth / 360)\n",
        "\n",
        "    y = ((1 - math.log(math.tan(lat * math.pi / 180) + 1 / math.cos(lat * math.pi / 180)) / math.pi) / 2) * mapHeight\n",
        "\n",
        "    return (x, y)\n",
        "\n",
        "\n",
        "\n",
        "def pointToLatLng(mapWidth, mapHeight, x, y):\n",
        "    lng = x / mapWidth * 360 - 180\n",
        "    n = math.pi - 2 * math.pi * y / mapHeight\n",
        "    lat = 180 / math.pi * math.atan(0.5 * (math.exp(n) - math.exp(-n)))\n",
        "\n",
        "\n",
        "    return (lat, lng)\n",
        "\n",
        "\n",
        "\n",
        "def getImageBounds(lat, lng, zoom):\n",
        "    picHeight = 640\n",
        "    picWidth = 640\n",
        "\n",
        "\n",
        "    mapHeight = 256\n",
        "    mapWidth = 256\n",
        "\n",
        "\n",
        "    xScale = math.pow(2, zoom) / (picWidth / mapWidth)\n",
        "    yScale = math.pow(2, zoom) / (picHeight / mapWidth)\n",
        "\n",
        "\n",
        "    centreX, centreY = latLngToPoint(mapWidth, mapHeight, lat, lng)\n",
        "\n",
        "\n",
        "    southWestX = centreX - (mapWidth / 2) / xScale\n",
        "    southWestY = centreY + (mapHeight / 2) / yScale\n",
        "    SWlat, SWlng = pointToLatLng(mapWidth, mapHeight, southWestX, southWestY)\n",
        "\n",
        "\n",
        "    northEastX = centreX + (mapWidth / 2) / xScale\n",
        "    northEastY = centreY - (mapHeight / 2) / yScale\n",
        "    NElat, NElng = pointToLatLng(mapWidth, mapHeight, northEastX, northEastY)\n",
        "\n",
        "\n",
        "    return [SWlat, SWlng, NElat, NElng]"
      ],
      "metadata": {
        "id": "u4GgC11B4_cO"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all nessessary functions established, the next step involves defining a list that includes all relevant land-use types. Since each node of the image data has an assigned type, we can employ a pre-defined set for land usages to ease this process."
      ],
      "metadata": {
        "id": "QgQ9kf5X6HOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list with all landuses\n",
        "dictionary_landuse = [\n",
        "    \"commercial\",\n",
        "    \"construction\",\n",
        "    \"education\",\n",
        "    \"fairground\",\n",
        "    \"industrial\",\n",
        "    \"residential\",\n",
        "    \"retail\",\n",
        "    \"institutional\",\n",
        "    \"aquaculture\",\n",
        "    \"allotments\",\n",
        "    \"farmland\",\n",
        "    \"farmyard\",\n",
        "    \"paddy\",\n",
        "    \"animal_keeping\",\n",
        "    \"flowerbed\",\n",
        "    \"forest\",\n",
        "    \"greenhouse_horticulture\",\n",
        "    \"meadow\",\n",
        "    \"orchard\",\n",
        "    \"plant_nursery\",\n",
        "    \"vineyard\",\n",
        "    \"basin\",\n",
        "    \"salt_pond\",\n",
        "    \"brownfield\",\n",
        "    \"cemetery\",\n",
        "    \"depot\",\n",
        "    \"garages\",\n",
        "    \"grass\",\n",
        "    \"greenfield\",\n",
        "    \"landfill\",\n",
        "    \"military\",\n",
        "    \"port\",\n",
        "    \"quarry\",\n",
        "    \"railway\",\n",
        "    \"recreation_ground\",\n",
        "    \"religious\",\n",
        "    \"village_green\",\n",
        "    \"winter_sports\",\n",
        "]"
      ],
      "metadata": {
        "id": "xKecKSZ26fRL"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our decision was to train the model on 5000 images."
      ],
      "metadata": {
        "id": "-CD5kuTO6gNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function concludes the data generation phase by generating a set of 5000 images for a specified number of cities. For each city, it randomly selects coordinates associated to various land uses, checks for data presence in the specified area, and downloads images for those coordinates. This marks the final step in preparing the dataset for subsequent model training.\n"
      ],
      "metadata": {
        "id": "_coDbUkmGob-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate a set of images for n cities\n",
        "def get_random_samples(n=1):\n",
        "    # loop to do multiple cities\n",
        "\n",
        "    for x in range(n):\n",
        "\n",
        "        # get a random city\n",
        "        city = get_random_city()\n",
        "\n",
        "        # get the coordinates for one of all the different landuses\n",
        "        coordinates = get_coordinates_for_each_landuse(city)\n",
        "\n",
        "        try:\n",
        "\n",
        "            # loop over each coordinate\n",
        "            for coordinate in coordinates:\n",
        "\n",
        "                try:\n",
        "                    # check if data is actually present in the given area\n",
        "                    south, east, north, west = getImageBounds(\n",
        "                        float(coordinate[0]), float(coordinate[1]), 16\n",
        "                    )\n",
        "                    landuse = ox.features_from_bbox(\n",
        "                        north, south, east, west, tags={\"landuse\": True}\n",
        "                    )\n",
        "                    # download the image of the given coordinate\n",
        "                    download_images(coordinate)\n",
        "\n",
        "                except:\n",
        "                    print(\"No landuse data was found\")\n",
        "\n",
        "        except:\n",
        "            print(\"Coordinates were returned empty\")"
      ],
      "metadata": {
        "id": "VvBBwuQw6qw7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial count of how much data is already in the folder\n",
        "datanames = os.listdir(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\")\n",
        "\n",
        "# loop as long as less then 5000 examples\n",
        "while len(datanames) < 5000:\n",
        "\n",
        "    get_random_samples()\n",
        "\n",
        "    datanames = os.listdir(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\")"
      ],
      "metadata": {
        "id": "NEz8B4cm6u1a",
        "outputId": "421bd597-82e7-47d7-efd5-3b4edbfb9ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Data/Images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-14e22954b45c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initial count of how much data is already in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatanames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Data\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loop as long as less then 5000 examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatanames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Data/Images'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Training the model**"
      ],
      "metadata": {
        "id": "H01sVphrIWlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Combining satellite and OpenStreetMap data**"
      ],
      "metadata": {
        "id": "e7DkvI5vPpZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have obtained the necessary image data, we shifts to training the model for our specific objectives. As the model identifies various landuse types present in our images, our initial step involves assigning a color for each different landuse type to ease the evaluation. Due to certain steps in the model training process not accepting Hexcode colors, we additional assigned each land-use type a unique integer identifier."
      ],
      "metadata": {
        "id": "JiTqNu9HIrNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map each landuse to a distinct color\n",
        "landuse_mapped_hex = {\n",
        "    \"not_assigned\": \"#FFFFFF\",\n",
        "    \"commercial\": \"#2F4F4F\",\n",
        "    \"construction\": \"#556B2F\",\n",
        "    \"education\": \"#A0522D\",\n",
        "    \"fairground\": \"#006400\",\n",
        "    \"industrial\": \"#8B0000\",\n",
        "    \"residential\": \"#808000\",\n",
        "    \"retail\": \"#483D8B\",\n",
        "    \"institutional\": \"#778899\",\n",
        "    \"aquaculture\": \"#BC8F8F\",\n",
        "    \"allotments\": \"#008B8B\",\n",
        "    \"farmland\": \"#00008B\",\n",
        "    \"farmyard\": \"#32CD32\",\n",
        "    \"paddy\": \"#DAA520\",\n",
        "    \"animal_keeping\": \"#8FB88F\",\n",
        "    \"flowerbed\": \"#8B008B\",\n",
        "    \"forest\": \"#B03060\",\n",
        "    \"greenhouse_horticulture\": \"#FF0000\",\n",
        "    \"meadow\": \"#FF8C00\",\n",
        "    \"orchard\": \"#FFFF00\",\n",
        "    \"plant_nursery\": \"#0000CD\",\n",
        "    \"vineyard\": \"#40E0D0\",\n",
        "    \"basin\": \"#00FF00\",\n",
        "    \"salt_pond\": \"#DC143C\",\n",
        "    \"brownfield\": \"#00BFFF\",\n",
        "    \"cemetery\": \"#A020F0\",\n",
        "    \"depot\": \"#F08080\",\n",
        "    \"garages\": \"#ADFF2F\",\n",
        "    \"grass\": \"#DA70D6\",\n",
        "    \"greenfield\": \"#FF7F50\",\n",
        "    \"landfill\": \"#FF00FF\",\n",
        "    \"military\": \"#F0E68C\",\n",
        "    \"port\": \"#6495ED\",\n",
        "    \"quarry\": \"#DDA0DD\",\n",
        "    \"railway\": \"#B0E0E6\",\n",
        "    \"recreation_ground\": \"#90EE90\",\n",
        "    \"religious\": \"#FF1493\",\n",
        "    \"village_green\": \"#7B68EE\",\n",
        "    \"winter_sports\": \"#FFDAB9\",\n",
        "}"
      ],
      "metadata": {
        "id": "2el0fnSfK_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map each color to a integer value that later represents the individual ids\n",
        "color_mapped_to_int = {\n",
        "    \"255255255\": 0,\n",
        "    \"477979\": 1,\n",
        "    \"8510747\": 2,\n",
        "    \"1608245\": 3,\n",
        "    \"01000\": 4,\n",
        "    \"13900\": 5,\n",
        "    \"1281280\": 6,\n",
        "    \"7261139\": 7,\n",
        "    \"119136153\": 8,\n",
        "    \"188143143\": 9,\n",
        "    \"0139139\": 10,\n",
        "    \"00139\": 11,\n",
        "    \"5020550\": 12,\n",
        "    \"21816532\": 13,\n",
        "    \"143188143\": 14,\n",
        "    \"1390139\": 15,\n",
        "    \"1764896\": 16,\n",
        "    \"25500\": 17,\n",
        "    \"2551400\": 18,\n",
        "    \"2552550\": 19,\n",
        "    \"00205\": 20,\n",
        "    \"64224208\": 21,\n",
        "    \"02550\": 22,\n",
        "    \"2202060\": 23,\n",
        "    \"0191255\": 24,\n",
        "    \"16032240\": 25,\n",
        "    \"240128128\": 26,\n",
        "    \"17325547\": 27,\n",
        "    \"218112214\": 28,\n",
        "    \"25512780\": 29,\n",
        "    \"2550255\": 30,\n",
        "    \"240230140\": 31,\n",
        "    \"100149237\": 32,\n",
        "    \"221160221\": 33,\n",
        "    \"176224230\": 34,\n",
        "    \"144238144\": 35,\n",
        "    \"25520147\": 36,\n",
        "    \"123104238\": 37,\n",
        "    \"255218185\": 38,\n",
        "}"
      ],
      "metadata": {
        "id": "TFQWzlDYLFYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Up to this point, our images are essentially snapshots of map data. To integrate our land-usages into the maps, we developed a function that determines the size of the images and reads the land-use type of each node. It can then apply the corresponding color to each node, therefore generating the initial plot for one of our map images."
      ],
      "metadata": {
        "id": "rvcPxhWXL0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the openstreetmap data and save it as a .png\n",
        "def plot_data(coordinates, zoom):\n",
        "    # get the bounding box form the given coordinates and zoom level\n",
        "    south, east, north, west = getImageBounds(\n",
        "        float(coordinates[0]), float(coordinates[1]), zoom\n",
        "    )\n",
        "\n",
        "    # query openstreetmap for all landuse in the boundingbox\n",
        "    landuse = ox.features_from_bbox(north, south, east, west, tags={\"landuse\": True})\n",
        "\n",
        "    # create a plot\n",
        "    fig, ax = plt.subplots(figsize=(8, 8), dpi=104)\n",
        "\n",
        "    # for each landuse in the queried data, color the respective area in the earlier specified color\n",
        "    for type in landuse[\"landuse\"].unique():\n",
        "        # filter so wrongly labeled data doesn't cause any issues (instances of this can be seen on the openstreetmap key wiki)\n",
        "        if type in dictionary_landuse:\n",
        "            # filter for the currently selected landuse and plot it.\n",
        "            # turning of antialising and edgecolor is important as to not cause the creation of new colors on the edges of the areas that weren't assigned to a landuse\n",
        "            landuse.loc[landuse[\"landuse\"] == type].plot(\n",
        "                ax=ax,\n",
        "                color=landuse_mapped_hex[type],\n",
        "                antialiased=False,\n",
        "                edgecolor=\"none\",\n",
        "            )\n",
        "    plt.xlim(east, west)\n",
        "    plt.ylim(south, north)\n",
        "    # turn of the axis, as to not save it in the image file\n",
        "    plt.axis(\"off\")\n",
        "    # save the image file under the specified path\n",
        "    fig.savefig(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\", bbox_inches=\"tight\", pad_inches=0,)\n",
        "    # close the plot to reduce memory usage\n",
        "    plt.close()\n",
        "    # open the image again\n",
        "    rgb_converted = Image.open(os.getcwd()+ os.sep+ \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")\n",
        "    # convert it from rgba to rgb as we are not using the alpha values\n",
        "    rgb_converted = rgb_converted.convert(\"RGB\")\n",
        "    # save it again\n",
        "    rgb_converted.save(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")"
      ],
      "metadata": {
        "id": "2HkVdoGyNB48"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The masks required for the eventual training of the model are based on these initial plots mentioned above. With similar aproaches involving defining bounds and color distribution, we create the mask to our initial map image plot."
      ],
      "metadata": {
        "id": "dI66qPMONXeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create numpy masks form the .png masks\n",
        "def generate_mask(coordinates):\n",
        "    # create 640x640 array filled with zeros\n",
        "    raster = np.zeros((640, 640))\n",
        "    # load the specified image file\n",
        "    image = Image.open(\n",
        "        os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")\n",
        "    # convert the image to a numpy array for way faster processing times (2min vs 1sec for mask creation)\n",
        "    image_array = np.array(image)\n",
        "    # go through each pixel and map the pixelvalue to the corresponding unique id\n",
        "    for line in range(640):\n",
        "        for column in range(640):\n",
        "            # get the value of the pixel in the image\n",
        "            pixelvalue = image_array[line, column]\n",
        "            # convert it to a string, because arrays can't be keys of dicitionaries, the easiest way to map the pixelvalues is by using their value as a string as keys\n",
        "            pixelvalue = map(str, pixelvalue)\n",
        "            colorvalue = \"\".join(pixelvalue)\n",
        "            # map the pixelvalue to the correspoding id\n",
        "            raster[line, column] = color_mapped_to_int[str(colorvalue)]\n",
        "    # save the numpy array using savez_compress to reduce the used storage space (3000 kb vs 10kb)\n",
        "    np.savez_compressed(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".npz\", raster,)\n",
        "    # delete the used picture\n",
        "    os.remove(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")"
      ],
      "metadata": {
        "id": "KDBYeFNSOqT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can then be looped through all the images available. In our case this porcess runs 5000 times."
      ],
      "metadata": {
        "id": "TRCxjKoHOuE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to loop over all satellite pictures and generate their masks\n",
        "def create_all_masks():\n",
        "    # get all filenames in Data/Images\n",
        "    datanames = os.listdir(os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\")\n",
        "    # loop over all names\n",
        "    for dataname in datanames:\n",
        "        # extract coordinates out of filename\n",
        "        coordinates = dataname.rstrip(\".png\").split(\"_\")\n",
        "        # check if numpy data already exists\n",
        "        if not os.path.exists(os.getcwd() + os.sep + \"Data\" + os.sep + \"Masks\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".npz\"):\n",
        "            coordinates = tuple(coordinates)\n",
        "            try:\n",
        "                # plot the data\n",
        "                plot_data(coordinates, 16)\n",
        "                # generate the numpy mask\n",
        "                generate_mask(coordinates)\n",
        "            except:\n",
        "                # if errors occur delte the problematic satellite image\n",
        "                print(\"An error occured while trying to generate the masks\")\n",
        "                os.remove( os.getcwd() + os.sep + \"Data\" + os.sep + \"Images\" + os.sep + str(coordinates[0]) + \"_\" + str(coordinates[1]) + \".png\")"
      ],
      "metadata": {
        "id": "FYIt5DHUPSPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_all_masks()"
      ],
      "metadata": {
        "id": "DyQKYwdYPUsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In diesem Block wird eine Funktion create_all_masks definiert, die alle erforderlichen Schritte für die Erzeugung von Masken für Satellitenbilder durchführt. Hier sind die Hauptpunkte des Codes:\n",
        "\n",
        "    Alle Dateinamen im Ordner \"Data/Images\" werden abgerufen.\n",
        "    Eine Schleife durchläuft alle Dateinamen.\n",
        "    Die Koordinaten werden aus dem Dateinamen extrahiert.\n",
        "    Es wird überprüft, ob bereits Numpy-Daten vorhanden sind.\n",
        "    Wenn keine Numpy-Daten vorhanden sind, wird versucht, die Funktionen plot_data (Erzeugung von Plotdaten) und generate_mask (Erzeugung von Masken) für die gegebenen Koordinaten aufzurufen.\n",
        "    Bei auftretenden Fehlern wird das betreffende Satellitenbild gelöscht.\n",
        "    Diese Schritte werden für alle verfügbaren Satellitenbilder durchgeführt, in diesem Fall 5000 Mal.\n",
        "\n",
        "Insgesamt ermöglicht diese Funktion die automatisierte Generierung von Masken für alle verfügbaren Satellitenbilder, die im Modelltraining verwendet werden sollen."
      ],
      "metadata": {
        "id": "zpEL-sNtKN40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2. Creating the Dataloader**"
      ],
      "metadata": {
        "id": "T_t5PAvMPz62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataloader process aligns to that from the lecture. Key points in our case are:\n",
        "\n",
        "\n",
        "1.  **Mask Restucturing**: It is essential to reorganize the masks stored as numpy files to make them compatible with the datablock's y_coordinate.\n",
        "2.   **Data Splitter**: We decided on using the RandomSplitter to split our images into train and test data.\n",
        "3.   **GPU Setup**: Due to using pytorch and cuda we can manually setup the process to be ran on the GPU instead of the CPU\n",
        "\n"
      ],
      "metadata": {
        "id": "Bn3kktVISkHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get the mask given an image\n",
        "def get_mask(x):\n",
        "    # load the numpyfile\n",
        "    numpyfile = np.load(mask_path / f\"{x.stem}.npz\")\n",
        "    # access the data stored in the compressed file format\n",
        "    data = numpyfile.f.arr_0\n",
        "    # close the file as to save memory\n",
        "    numpyfile.close()\n",
        "    # return the extracted mask\n",
        "    return data"
      ],
      "metadata": {
        "id": "nMbuPbCKTPiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the datablock\n",
        "satellite_block = DataBlock(\n",
        "    blocks=(ImageBlock, MaskBlock(codes=dictionary_landuse)),\n",
        "    get_items=get_image_files,\n",
        "    get_y=get_mask,\n",
        "    splitter=RandomSplitter(),\n",
        "    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n",
        "    batch_tfms=aug_transforms(),\n",
        ")"
      ],
      "metadata": {
        "id": "C9q3GWQ5UBrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if pytorch and cuda is setup correctly enable the dataloader to run on the gpu by passing it the cuda device (Installation of cuda https://developer.nvidia.com/cuda-downloads is necessary)\n",
        "if torch.cuda.is_available():\n",
        "    dataloader = satellite_block.dataloaders(\n",
        "        image_path, bs=32, device=torch.device(\"cuda\")\n",
        "    )\n",
        "\n",
        "\n",
        "else:\n",
        "    dataloader = satellite_block.dataloaders(image_path, bs=32)"
      ],
      "metadata": {
        "id": "a-uQHAxDUsul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Training the model"
      ],
      "metadata": {
        "id": "RPzZ8Kz9UOAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In diesem Abschnitt wurde ein U-Net-Modell mit ResNet34-Architektur trainiert, wobei drei verschiedene Lernraten in aufeinanderfolgenden Durchläufen verwendet wurden. Zuerst wurde eine dynamisch angepasste Lernrate mit frühzeitigem Stoppen eingesetzt. Im zweiten Durchlauf wurde das Modell unfreezed, und eine erneut angepasste Lernrate wurde verwendet. Im dritten Durchlauf wurde eine feste Lernrate angewendet. Die besten Modelle wurden während des Trainings gespeichert, und am Ende wurde das finale Modell exportiert."
      ],
      "metadata": {
        "id": "YpTN-5obGtZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learner text"
      ],
      "metadata": {
        "id": "kVDSSV_tUxvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the unet learner, the metrics, also enable self attention, set mish as the activation function and pass ranger as the optimization function\n",
        "learner = unet_learner(\n",
        "    dataloader,\n",
        "    resnet34,\n",
        "    metrics=[dice_wo_bg, Dice],\n",
        "    self_attention=True,\n",
        "    act_cls=Mish,\n",
        "    opt_func=ranger,\n",
        ")"
      ],
      "metadata": {
        "id": "TmNaNeY9UwsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "qa3FFNRX7qkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code block utilizes the lr_find() method to identify the most effective learning rate for the learner. This is a crucial step for achieving efficient convergence during subsequent training."
      ],
      "metadata": {
        "id": "oQvc7QgZ7srz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find learning rate\n",
        "lr = learner.lr_find()"
      ],
      "metadata": {
        "id": "u9RRYXJ17lTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block executes the first iteration of model training. The model is trained for a maximum of 20 epochs using the learning rate derived before. Early stopping, based on the custom metric \"dice_wo_bg,\" is applied, and the model is saved as \"first_pass\" if an improvement in the metric is observed."
      ],
      "metadata": {
        "id": "04dv13bU73h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first pass model training, with early stopping after seeing no improvment in the custom defined metric for 4 epochs, train max of 20 epochs\n",
        "learner.fit_one_cycle(\n",
        "    20,\n",
        "    lr_max=lr.valley,\n",
        "    cbs=[\n",
        "        EarlyStoppingCallback(\n",
        "            monitor=\"dice_wo_bg\", min_delta=0.01, patience=4, comp=np.greater\n",
        "        ),\n",
        "        SaveModelCallback(\n",
        "            monitor=\"dice_wo_bg\", min_delta=0.01, comp=np.greater, fname=\"first_pass\"\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "0MtGnQlD73oq",
        "outputId": "1064b58b-db07-400a-ef33-2779d5c0e7ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'learner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-b822785773e7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# first pass model training, with early stopping after seeing no improvment in the custom defined metric for 4 epochs, train max of 20 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m learner.fit_one_cycle(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     cbs=[\n",
            "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section block loads the machine learning model with weights saved after the initial training pass (\"first_pass\") and displays predictions for a subset of examples using the show_results method."
      ],
      "metadata": {
        "id": "8A3yUdBC78RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the lerner and show some examples\n",
        "learner = learner.load(\"first_pass\")\n",
        "learner.show_results(max_n=5)"
      ],
      "metadata": {
        "id": "2_7Tx-Ii78YV",
        "outputId": "a5e50d93-d4ad-40e3-fcd8-7bca2b0add4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'learner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-11afc5e2e79c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the lerner and show some examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_pass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block unfreezes the previously trained model to allow fine-tuning. Then, it determines a new optimal learning rate for further training using the lr_find() method."
      ],
      "metadata": {
        "id": "1PliRzOrBbne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unfreeze the model and find a new learning rate\n",
        "learner.unfreeze()\n",
        "lr = learner.lr_find()"
      ],
      "metadata": {
        "id": "6P-T1VuJBbu8",
        "outputId": "560b9842-bfab-44c4-a3c6-f777a0750fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'learner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-ebcaeb804b0b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# unfreeze the model and find a new learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block executes the second iteration of model training. It retrains the model again for 20 epochs, but adjusting the learning rate within a specified range based on the new optimal rate. Also early stopping is applied again in case of insufficient improvement."
      ],
      "metadata": {
        "id": "8_1I-UkNBcTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model again for 20 epochs unless there is no improvment seen for 4 epochs\n",
        "learner.fit_one_cycle(\n",
        "    20,\n",
        "    lr_max=slice(lr.valley / 10, lr.valley * 10),\n",
        "    cbs=[\n",
        "        EarlyStoppingCallback(\n",
        "            monitor=\"dice_wo_bg\", min_delta=0.01, patience=4, comp=np.greater\n",
        "        ),\n",
        "        SaveModelCallback(monitor=\"dice_wo_bg\", comp=np.greater, fname=\"second_pass\"),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "2UjGcBktBcY_",
        "outputId": "685f5aa5-dc42-43d9-e2eb-8b4f1e677429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'learner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-9730485659b3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train the model again for 20 epochs unless there is no improvment seen for 4 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m learner.fit_one_cycle(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalley\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalley\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     cbs=[\n",
            "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code loads the machine learning model with weights saved from the second training pass ('second_pass') and generates predictions for a subset of examples using the show_results method."
      ],
      "metadata": {
        "id": "D30k4hJJBd11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and show some examples\n",
        "learner = learner.load(\"second_pass\")\n",
        "learner.show_results(max_n=5)"
      ],
      "metadata": {
        "id": "i2tTU15qBd7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfreezing the previously trained model to allow for fine-tuning. With that, a new optimal learning rate can be determined."
      ],
      "metadata": {
        "id": "Dd5vEhe2Olf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unfreeze the model and find a new learning rate\n",
        "learner.unfreeze()\n",
        "lr = learner.lr_find()"
      ],
      "metadata": {
        "id": "7jACA0_MBgkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retraining the model for a third time with the specified settings, utilizing the optimal learning rate range and implementing early stopping based on a custom metric."
      ],
      "metadata": {
        "id": "bDEjDqs_Op0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model for a thrid time with the same settings\n",
        "learner.fit_one_cycle(\n",
        "    20,\n",
        "    lr_max=slice(lr.valley / 10, lr.valley * 10),\n",
        "    cbs=[\n",
        "        EarlyStoppingCallback(\n",
        "            monitor=\"dice_wo_bg\", min_delta=0.01, patience=4, comp=np.greater\n",
        "        ),\n",
        "        SaveModelCallback(\n",
        "            monitor=\"dice_wo_bg\", comp=np.greater, fname=\"finetuned_model\"\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "PWcEYv0IBjJw",
        "outputId": "4cf3eb1f-bf38-420a-b708-fc2ecfcd5ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'learner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-10991a1db472>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model for a thrid time with the same settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m learner.fit_one_cycle(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalley\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalley\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     cbs=[\n",
            "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code loads the model with weights saved after the third training pass (\"finetuned_model\") and displays predictions for a subset of examples."
      ],
      "metadata": {
        "id": "dWb241e3Bkv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and show some examples\n",
        "learner = learner.load(\"finetuned_model\")\n",
        "learner.show_results(max_n=5)"
      ],
      "metadata": {
        "id": "mZ0vv6TCBk1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[After the third training] Exporting the final machine learning model and saving it as a Pickle file named \"model.pkl.\"\n"
      ],
      "metadata": {
        "id": "FJoCz0PQOtxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# export the final model\n",
        "learner = learner.load(\"finetuned_model\")\n",
        "learner.export(fname=\"model.pkl\")"
      ],
      "metadata": {
        "id": "MAoNdXpjBmMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}